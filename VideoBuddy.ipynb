{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/kerasv6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import face_recognition\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load video as iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imutils.video import FileVideoStream\n",
    "#from imutils.video import FPS\n",
    "#import numpy as np\n",
    "#import argparse\n",
    "#import imutils\n",
    "#import time\n",
    "#import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.pyimagesearch.com/2017/02/06/faster-video-file-fps-with-cv2-videocapture-and-opencv/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path video to analyze\n",
    "path = './Videos/sad_affleck.mp4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoBuddy:\n",
    "    def __init__(self, path, fps):\n",
    "        # Create video instance\n",
    "        self.stream = cv2.VideoCapture(path)\n",
    "        # fps is the downsampled fps to apply Yolo\n",
    "        self.fps = fps\n",
    "        # Set threaded fps (Adrian R)\n",
    "        self.stopped = False\n",
    "        self.Q = Queue(maxsize=128)\n",
    "        \n",
    "    def start(self):\n",
    "        thread = Thread(target=self.update, args=())\n",
    "        thread.daemon = True\n",
    "        thread.start()\n",
    "        return self\n",
    "        \n",
    "    def update(self):\n",
    "        while True:\n",
    "            \n",
    "            if self.stopped:\n",
    "                return\n",
    "            \n",
    "            if not self.Q.full():\n",
    "                ret, frame = self.stream.read()\n",
    "                \n",
    "            if not ret:\n",
    "                self.stop()\n",
    "                return\n",
    "            self.Q.put(frame)\n",
    "    \n",
    "    def read(self):\n",
    "        return self.Q.get()\n",
    "    \n",
    "    def more(self):\n",
    "        return self.Q.qsize() > 0\n",
    "    \n",
    "    def stop(self):\n",
    "        self.stopped = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Queue' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2a4196141707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoBuddy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Generate the thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-df4769f11a97>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, fps)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Set threaded fps (Adrian R)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Queue' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the instance\n",
    "vid = VideoBuddy(path, 120)\n",
    "\n",
    "# Generate the thread\n",
    "vid.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-4ae16a77bb64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0mvid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# grab the frame from the threaded video file stream, resize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# it, and convert it to grayscale (while still retaining 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# channels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-2b282ba383d8>\u001b[0m in \u001b[0;36mmore\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/kerasv6/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mqsize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Raises NotImplementedError on Mac OSX because of broken sem_getvalue()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maxsize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_semlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while vid.more():\n",
    "    # grab the frame from the threaded video file stream, resize\n",
    "    # it, and convert it to grayscale (while still retaining 3\n",
    "    # channels)\n",
    "    frame = vid.read()\n",
    "    frame = imutils.resize(frame, width=450)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = np.dstack([frame, frame, frame])\n",
    " \n",
    "    # display the size of the queue on the frame\n",
    "    cv2.putText(frame, \"Queue Size: {}\".format(fvs.Q.qsize()),\n",
    "        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\t\n",
    " \n",
    "    # show the frame and update the FPS counter\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.waitKey(1)\n",
    "    vid.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_utils import read_classes, read_anchors, generate_colors, preprocess_image, draw_boxes\n",
    "from yad2k.models.keras_yolo import yolo_head, yolo_eval\n",
    "\n",
    "from wide_resnet import WideResNet\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast threaded implementation does not work on MacOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VideoBuddy:\n",
    "    def __init__(self, path, fps=120, batch_size=128, verbosity=True, threshold=0.3):\n",
    "        # Create video instance\n",
    "        self.stream = cv2.VideoCapture(path)\n",
    "        # fps is the downsampled fps to apply Yolo\n",
    "        self.fps = fps\n",
    "        self.stopped = False\n",
    "        self.image_counter = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_counter = 0\n",
    "        self.verbosity = verbosity\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def set_age_gender_model(self):\n",
    "        # load Age and gender model and weights\n",
    "        img_size = 64\n",
    "        depth = 16\n",
    "        k = 8\n",
    "        weight_file = 'pretrained_models/weights.18-4.06.hdf5'\n",
    "        self.ag_model = WideResNet(img_size, depth=depth, k=k)()\n",
    "        self.ag_model.load_weights(weight_file)\n",
    "\n",
    "    def set_emotion_model(self):    \n",
    "        # Load \n",
    "        emotion_model_path = 'pretrained_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n",
    "        self.emotion_model = load_model(emotion_model_path)\n",
    "        \n",
    "    def init_yolo(self):\n",
    "        # Init Yolo configuration\n",
    "        self.class_names = read_classes(\"model_data/coco_classes.txt\")\n",
    "        self.anchors = read_anchors(\"model_data/yolo_anchors.txt\")\n",
    "        #Load the pretrained model\n",
    "        self.yolo = load_model(\"model_data/yolo.h5\")\n",
    "        #Convert final layer features to bounding box parameters\n",
    "        self.yolo_outputs = yolo_head(self.yolo.output, self.anchors, len(self.class_names))\n",
    "        width = np.array(416, dtype=float)\n",
    "        height = np.array(416, dtype=float)\n",
    "        self.image_shape = (width, height)\n",
    "        self.boxes, self.scores, self.classes = yolo_eval(self.yolo_outputs, self.image_shape)\n",
    "        self.sess = K.get_session()\n",
    "        return\n",
    "        \n",
    "    def read(self):\n",
    "        # Check if the video is no at the end\n",
    "        if self.stopped:\n",
    "            return\n",
    "        # get current frame\n",
    "        success, frame = self.stream.read()\n",
    "        # Check if is not the end of the video\n",
    "        if not success:\n",
    "            self.stopped = True\n",
    "            print('Video Succesfully Analysed. {} images processed'.format(self.image_counter))\n",
    "            return\n",
    "        # Update image counter\n",
    "        self.image_counter = self.image_counter + 1\n",
    "        return frame\n",
    "    \n",
    "    def batch_gen(self):\n",
    "        # Read and generate a single batch\n",
    "        self.batch = [self.read() for index in range(self.batch_size)]\n",
    "        return np.array(self.batch)\n",
    "    \n",
    "    def analyze(self):\n",
    "        # Generate batchs until the end of the video\n",
    "        self.yolo_predictions = []\n",
    "        \n",
    "        while not self.stopped:\n",
    "            # Generate one batch\n",
    "            self.current_batch = self.batch_gen()\n",
    "            #self.predictions = self.object_detection()\n",
    "            self.predictions = self.face_detection()\n",
    "        \n",
    "            self.yolo_predictions.append(self.predictions)    \n",
    "            \n",
    "            # Show some info\n",
    "            if self.verbosity and not self.stopped:            \n",
    "                self.batch_counter = self.batch_counter + 1\n",
    "                print('Analyzing batch {}. Elapsed time ='.format(self.batch_counter))\n",
    "                \n",
    "        return self.yolo_predictions\n",
    "    \n",
    "    def object_detection(self):\n",
    "        # Downsample batch to approx 1 image per second\n",
    "        self.downsampled_batch = self.current_batch[::24]\n",
    "        # Check is there is not empty elements in the list\n",
    "        for frame in self.downsampled_batch:\n",
    "            if frame is None:\n",
    "                self.stopped = True\n",
    "        # Resize images in batch, convert to numpy array and normalize\n",
    "        if not self.stopped:\n",
    "            self.downsampled_batch = [cv2.resize(frame, (416,416)) for frame in self.downsampled_batch]\n",
    "            self.downsampled_batch = np.array(self.downsampled_batch, dtype='float32')\n",
    "            self.downsampled_batch /= 255.\n",
    "            # Call Yolo predictions with the dowsampled batch\n",
    "            self.out_scores, self.out_boxes, self.out_classes = self.sess.run([self.scores, self.boxes, self.classes],\n",
    "                                                    feed_dict= {self.yolo.input:self.downsampled_batch,\n",
    "                                                                K.learning_phase(): 0})\n",
    "        return self.out_classes\n",
    "    \n",
    "    def face_detection(self):\n",
    "        # Convert frame to small frame\n",
    "        self.total_faces = []\n",
    "        self.total_emotions = []\n",
    "        self.total_age = []\n",
    "        self.total_gender = []\n",
    "        self.downsampled_batch = self.current_batch[::24]\n",
    "        # Check is there is not empty elements in the list\n",
    "        for frame in self.downsampled_batch:\n",
    "            if frame is None:\n",
    "                self.stopped = True\n",
    "                \n",
    "        if not self.stopped:\n",
    "            self.downsampled_batch = [cv2.resize(frame, (0, 0), fx=0.25, fy=0.25) for frame in self.downsampled_batch]\n",
    "            self.downsampled_batch = [frame[:, :, ::-1] for frame in self.downsampled_batch]\n",
    "            self.face_locations = [face_recognition.face_locations(rgb_frame, model='hog',\n",
    "                                                               number_of_times_to_upsample=3) for rgb_frame in self.downsampled_batch]    \n",
    "\n",
    "            self.emotion_detection()\n",
    "        return\n",
    "    \n",
    "    def emotion_detection(self):\n",
    "        \n",
    "        self.face_emotions = []\n",
    "        for face in self.face_locations:\n",
    "            print(len(face))\n",
    "            # Keep the number of faces detected on the frames\n",
    "            self.total_faces.append(len(self.face_locations))\n",
    "        \n",
    "            print(len(self.face_locations))\n",
    "            # Get face coordinates\n",
    "            coord = []\n",
    "            for element in face:\n",
    "                top, right, bottom, left = element\n",
    "                coord.append([top, right, bottom, left])\n",
    "                \n",
    "            # Crop image\n",
    "            face_image = []\n",
    "            for index, frame in enumerate(self.downsampled_batch):\n",
    "                top, right, bottom, left = coord[index]\n",
    "                face_image.append(frame[top:bottom, left:right])\n",
    "                \n",
    "            #face_image = [frame[top:bottom, left:right] for frame in self.rgb_small_frames]\n",
    "            \n",
    "            # Pre-process image\n",
    "            color_img, gray_img = pre_processing(face_image)\n",
    "\n",
    "            # Predict emotion and gender\n",
    "            predicted_emotion = self.emotion_model.predict(gray_img)\n",
    "            #print(predicted_emotion)\n",
    "            # predict ages and genders of the detected faces\n",
    "            results = self.ag_model.predict(color_img)\n",
    "            predicted_gender = [result[0] for result in results]\n",
    "            age = np.arange(0, 101).reshape(101, 1)\n",
    "            predicted_age = [result[1].dot(age).flatten() for result in results]\n",
    "\n",
    "            # Keep local predictions for final analysis\n",
    "            self.total_emotions.append(np.argmax(predicted_emotion))\n",
    "            self.total_age.append(int(predicted_age))\n",
    "            self.total_gender.append(np.argmax(predicted_gender))\n",
    "\n",
    "            # Decode outputs\n",
    "            age_prediction = [str(age)[1:3] for age in predicted_age]\n",
    "            \n",
    "            #age_prediction =  #emotion_decoder(emotion_output, 0.30)\n",
    "            #gender_prediction = gender_decoder(predicted_gender)\n",
    "            gender_prediction = [gender_decoder(gender) for gender in predicted_gender]\n",
    "\n",
    "            #emotion_prediction = emotion_decoder(predicted_emotion)\n",
    "            emotion_prediction = [emotion_decoder(emotion) for emotion in predicted_emotion]\n",
    "\n",
    "            # Create text to add on the rectangle\n",
    "            prediction = emotion_prediction + '\\n' + gender_prediction + '\\n' + age_prediction\n",
    "\n",
    "            # Add predictions to rectangle\n",
    "            self.face_emotions.append(prediction)\n",
    "        \n",
    "        return self.face_emotions\n",
    "    \n",
    "    def annotate_frame(self):\n",
    "        for (top, right, bottom, left), emotion in zip(self.face_locations, self.face_emotions):\n",
    "            # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "            top *= 4\n",
    "            right *= 4\n",
    "            bottom *= 4\n",
    "            left *= 4\n",
    "\n",
    "            # Create a copy of the frame as overlay\n",
    "            # CREATE ANOTHER BATCH VERSION\n",
    "            overlay = frame.copy()\n",
    "\n",
    "            # Draw a box around the face\n",
    "            cv2.rectangle(overlay, (left, top), (right, bottom), (180, 0, 0), 2) #Blue\n",
    "\n",
    "            # Plot predictions\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            y0, dy = 0, 30\n",
    "            for i, line in enumerate(emotion.split('\\n')):\n",
    "                y = y0 + i*dy\n",
    "                cv2.putText(overlay, line, (left + 6, bottom - 6 + y), font, 1.0, (255, 255, 255), 1)\n",
    "            opacity = 0.5\n",
    "            cv2.addWeighted(overlay, opacity, frame, 1 - opacity, 0, frame)\n",
    "    \n",
    "    def gender_decoder(self):\n",
    "        self.net_output = self.predicted_gender\n",
    "        if np.max(self.net_output) > self.threshold:\n",
    "            probability = str(np.max(self.net_output))\n",
    "            if np.argmax(self.net_output) == 0:\n",
    "                prediction = 'female'\n",
    "            else:\n",
    "                prediction = 'male'\n",
    "        else:\n",
    "            prediction = ''\n",
    "            probability = ''\n",
    "\n",
    "        self.gender_pred = prediction + ' ' + probability[:5] \n",
    "        \n",
    "        return self.gender_pred\n",
    "    \n",
    "    \n",
    "    def emotion_decoder(self):\n",
    "    # Check if probability is higher than threshold\n",
    "        if np.max(self.net_output) >= self.threshold:\n",
    "            probability = str(np.max(self.net_output))\n",
    "            if (np.argmax(self.net_output) == 0):\n",
    "                prediction = 'Angry'\n",
    "            elif (np.argmax(self.net_output) == 1):\n",
    "                prediction = 'Disgust'\n",
    "            elif (np.argmax(self.net_output) == 2):\n",
    "                prediction = 'Fear'\n",
    "            elif (np.argmax(self.net_output) == 3):\n",
    "                prediction = 'Happy'\n",
    "            elif (np.argmax(self.net_output) == 4):\n",
    "                prediction = 'Sad'\n",
    "            elif (np.argmax(self.net_output) == 5):\n",
    "                prediction = 'Surprise'\n",
    "            else:\n",
    "                prediction = 'Neutral'\n",
    "        else:\n",
    "            # Do not label the box if you are not sure\n",
    "            prediction = ''\n",
    "            probability = ''\n",
    "        emotion_pred = prediction + ' ' + probability[:5]\n",
    "\n",
    "        return self.emotion_pred\n",
    "    \n",
    "\n",
    "    def pre_processing(self):\n",
    "        # Convert image to B&W\n",
    "        gray_frames = [cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) for frame in self.downsampled_batch]\n",
    "        gray_frames = [normalize_input(frame) for frame in gray_frames]\n",
    "        gray_frames = [cv2.resize(frame,(64,64), interpolation = cv2.INTER_CUBIC) for frame in gray_frames]\n",
    "        self.gray_frames = np.reshape(gray_frames,(-1,64,64,1))\n",
    "\n",
    "        # Color image\n",
    "        color_frames = [cv2.resize(frame,(64,64), interpolation = cv2.INTER_CUBIC) \n",
    "                        for frame in self.downsampled_batch]\n",
    "        \n",
    "        self.color_frames = np.reshape(color_frames,(-1, 64,64,3))\n",
    "        \n",
    "        return self.color_frames, self.gray_frames\n",
    "    \n",
    "    def normalize_input(self, v2=True):\n",
    "        x = self.gray_frames.astype('float32')\n",
    "        x = x / 255.0\n",
    "        if v2:\n",
    "            x = x - 0.5\n",
    "            self.normalized_batch = x * 2.0\n",
    "        return self.normalized_batch\n",
    "    \n",
    "    def reset_counter(self):\n",
    "        self.image_counter = 0\n",
    "        return self.image_counter\n",
    "    \n",
    "    def filter_artifacts(self):\n",
    "        return\n",
    "    \n",
    "    def find(self):\n",
    "        return\n",
    "    \n",
    "    def generate_video(self):\n",
    "        return\n",
    "    \n",
    "    def emotion_recognition(self):\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vid = VideoBuddy(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vid.init_yolo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add path\n",
    "vid.set_age_gender_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/kerasv6/lib/python3.6/site-packages/keras/models.py:318: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "# add path\n",
    "vid.set_emotion_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-20f4e673ffd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-106-6964154199c6>\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m#self.predictions = self.object_detection()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolo_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-6964154199c6>\u001b[0m in \u001b[0;36mface_detection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m                                                                number_of_times_to_upsample=3) for rgb_frame in self.downsampled_batch]    \n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memotion_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-6964154199c6>\u001b[0m in \u001b[0;36memotion_detection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mface_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsampled_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0mface_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred = vid.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0]),\n",
       " array([0, 0]),\n",
       " array([0, 0]),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([0, 0]),\n",
       " array([0, 0]),\n",
       " array([ 0,  0, 65]),\n",
       " array([0, 0]),\n",
       " array([0, 0]),\n",
       " array([0, 0]),\n",
       " array([0, 0]),\n",
       " array([0, 0]),\n",
       " array([0, 0]),\n",
       " array([0, 0]),\n",
       " array([0, 0]),\n",
       " array([0, 0])]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kerasv6)",
   "language": "python",
   "name": "kerasv6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
